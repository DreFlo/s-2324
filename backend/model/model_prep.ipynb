{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383ad5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638b7f6",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e1a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(size):\n",
    "    with open('resources/test_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    subset_dictionary = {key: data[key] for key in list(data.keys())[:size]}\n",
    "    \n",
    "    with open('resources/test_data_subset_' + str(size) + '.json', 'w') as file:\n",
    "        json.dump(subset_dictionary, file, indent=2)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96290395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '../resources/test_data_subset_100.json'\n",
    "dataframe_file_path = '../resources/data_subset_100.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f37c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data():\n",
    "    df = pd.read_csv(dataframe_file_path)\n",
    "    \n",
    "    print(\"Rows: \", len(df))\n",
    "    for col in df.columns:\n",
    "        print(col, \": \", (df[col].isnull().sum()/len(df)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d1fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_sec_last'] = None\n",
      "C:\\Users\\dluis\\AppData\\Local\\Temp\\ipykernel_53172\\2265969160.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[metric + '_third_last'] = None\n"
     ]
    }
   ],
   "source": [
    "with open(data_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "symbols = list(data.keys())\n",
    "\n",
    "metrics = list(data[symbols[0]]['key_metrics_by_form']['10-Q'].keys())\n",
    "\n",
    "df = pd.DataFrame(columns=['symbol'])\n",
    "\n",
    "df['stock_change_before'] = None\n",
    "df['buy'] = None\n",
    "\n",
    "for metric in metrics:\n",
    "    df[metric + '_last'] = None\n",
    "    df[metric + '_sec_last'] = None\n",
    "    df[metric + '_third_last'] = None\n",
    "\n",
    "for symbol in symbols:\n",
    "    dates = list(data[symbol]['facts_by_form']['10-Q']['fileds']) + list(data[symbol]['facts_by_form']['10-K']['fileds'])\n",
    "    dates = list(set(dates))\n",
    "    dates.sort()\n",
    "\n",
    "    if len(dates) < 6:\n",
    "        continue\n",
    "\n",
    "    encoded_symbol = LabelEncoder().fit_transform([symbol])[0]\n",
    "\n",
    "    start_date = datetime.strptime(dates[3], '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(dates[-2], '%Y-%m-%d')\n",
    "    random_dates = []\n",
    "\n",
    "    for _ in range(len(dates) // 2):\n",
    "        random_date = start_date + (end_date - start_date) * random.random()\n",
    "        random_dates.append(random_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    for date in random_dates:\n",
    "        date_dt = datetime.strptime(date, '%Y-%m-%d')\n",
    "        row_values = [encoded_symbol]\n",
    "\n",
    "        stock_dates = list(data[symbol]['stock_price_history'].keys())\n",
    "        stock_change_over = []\n",
    "        stock_change_before = []\n",
    "        for i, stock_date in enumerate(stock_dates):\n",
    "            stock_date_dt = datetime.strptime(stock_date, '%Y-%m-%d')\n",
    "\n",
    "            if (stock_date_dt - date_dt).days < 30 and (stock_date_dt - date_dt).days > 0:\n",
    "                stock_change_over.append(data[symbol]['stock_price_history'][stock_date]['close'] - data[symbol]['stock_price_history'][stock_dates[i-1]]['close'])\n",
    "            elif (stock_date_dt - date_dt).days > -30 and (stock_date_dt - date_dt).days < 0:\n",
    "                stock_change_before.append(data[symbol]['stock_price_history'][stock_date]['close'] - data[symbol]['stock_price_history'][stock_dates[i-1]]['close'])\n",
    "\n",
    "        if len(stock_change_over) == 0 or len(stock_change_before) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            row_values.append(statistics.median(stock_change_before))\n",
    "\n",
    "            median_over = statistics.median(stock_change_over)\n",
    "            if median_over > 0:\n",
    "                row_values.append(1)\n",
    "            else:\n",
    "                row_values.append(0)\n",
    "\n",
    "        dates_before = [_date for _date in dates if _date <= date]\n",
    "\n",
    "        for metric in metrics: \n",
    "            found_no = 0\n",
    "\n",
    "            for _date in dates_before[::-1]:\n",
    "                if found_no == 3:\n",
    "                    break\n",
    "\n",
    "                if _date in data[symbol]['key_metrics_by_form']['10-Q'][metric]:\n",
    "                    if type(data[symbol]['key_metrics_by_form']['10-Q'][metric][_date]) == dict:\n",
    "                        row_values.append(data[symbol]['key_metrics_by_form']['10-Q'][metric][_date]['val'])\n",
    "                        found_no += 1\n",
    "                        continue\n",
    "\n",
    "                if _date in data[symbol]['key_metrics_by_form']['10-K'][metric]:\n",
    "                    if type(data[symbol]['key_metrics_by_form']['10-K'][metric][_date]) == dict:\n",
    "                        row_values.append(data[symbol]['key_metrics_by_form']['10-K'][metric][_date]['val'])\n",
    "                        found_no += 1\n",
    "                        continue\n",
    "\n",
    "            while found_no < 3:\n",
    "                row_values.append(np.nan)\n",
    "                found_no += 1\n",
    "\n",
    "        df.loc[len(df)] = row_values\n",
    "\n",
    "df.dropna(how='all', inplace=True)\n",
    "\n",
    "df.to_csv(dataframe_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dc22d",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078c62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, model, X_test, y_test):\n",
    "    print(name + \":\")\n",
    "    print(\"Score: {:.4f}%\".format(model.score(X_test, y_test) * 100))\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab612c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataframe_file_path)\n",
    "    \n",
    "X = df.drop('buy', axis=1)\n",
    "y = df['buy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10067e6",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9666e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline lightGBM model::\n",
      "Score: 62.8571%\n",
      "ROC-AUC: 0.6227533984125535\n",
      "[[79 34]\n",
      " [44 53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.70      0.67       113\n",
      "         1.0       0.61      0.55      0.58        97\n",
      "\n",
      "    accuracy                           0.63       210\n",
      "   macro avg       0.63      0.62      0.62       210\n",
      "weighted avg       0.63      0.63      0.63       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_model('Baseline lightGBM model:', model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8f9b2",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27650d3",
   "metadata": {},
   "source": [
    "### Need to check which models to use and which params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6697a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 200 candidates, totalling 20000 fits\n",
      "LightGBM with gridsearch::\n",
      "Score: 59.5238%\n",
      "ROC-AUC: 0.5895903658425325\n",
      "[[75 38]\n",
      " [47 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.66      0.64       113\n",
      "         1.0       0.57      0.52      0.54        97\n",
      "\n",
      "    accuracy                           0.60       210\n",
      "   macro avg       0.59      0.59      0.59       210\n",
      "weighted avg       0.59      0.60      0.59       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [75,100,150,200],\n",
    "            'classifier__criterion': ['gini', 'entropy'],\n",
    "            'classifier__bootstrap' : [\"True\", \"False\"],\n",
    "            'classifier__max_depth' : [7,8,9,10],\n",
    "            'classifier__max_features': ['sqrt', 'log2'],\n",
    "            'classifier__n_jobs' : [5]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(multi_class='auto', max_iter=300),\n",
    "        'params': {\n",
    "            'classifier__solver': ['lbfgs', 'liblinear'],\n",
    "            'classifier__C': np.geomspace(1e-5, 1e5, num=20)\n",
    "        }\n",
    "    },\n",
    "    'knn': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [1,10,1],\n",
    "            'classifier__leaf_size': [20,40,1],\n",
    "            'classifier__p': [1,2],\n",
    "            'classifier__weights': ('uniform', 'distance'),\n",
    "            'classifier__metric': ('minkowski', 'chebyshev')\n",
    "        }\n",
    "    },\n",
    "    'lightGBM': {\n",
    "        'model': LGBMClassifier(objective=\"binary\", random_state=0),\n",
    "        'params': {\n",
    "            'classifier__num_leaves': np.arange(2, 10, 1),\n",
    "            'classifier__max_depth': np.arange(1, 6, 1),\n",
    "            'classifier__n_estimators': np.arange(50, 151, 25)\n",
    "        }\n",
    "        \n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params' : {\n",
    "            'classifier__criterion' : ['gini', 'entropy'],\n",
    "            'classifier__max_depth' : range(1, 20),\n",
    "            'classifier__min_samples_split' : range(1, 15),\n",
    "            'classifier__min_samples_leaf' : range(1, 10)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    \n",
    "kf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "\n",
    "scores = []\n",
    "for model_name, mp in model_params.items():\n",
    "    if model_name != 'lightGBM':\n",
    "        continue\n",
    "    #pipeline = Pipeline([('smote', SMOTE(random_state=42)), ('classifier', mp['model'])])\n",
    "    pipeline = Pipeline([('classifier', mp['model'])])\n",
    "    grid_search = GridSearchCV(pipeline,\n",
    "                            param_grid=mp['params'],\n",
    "                            return_train_score=False,\n",
    "                            cv=kf,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "\n",
    "#This just works if the for loop is only running for 'lightGBM' model\n",
    "lightGBM_model = LGBMClassifier(objective=\"binary\", random_state=0, max_depth=df.iloc[0]['best_params']['classifier__max_depth'], n_estimators=df.iloc[0]['best_params']['classifier__n_estimators'], num_leaves=df.iloc[0]['best_params']['classifier__num_leaves'])\n",
    "lightGBM_model.fit(X_train, y_train)\n",
    "\n",
    "test_model('LightGBM with gridsearch:', lightGBM_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa00d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc5161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b7111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56f9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
